#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ‰¹é‡æŠ“å– cclcat å¯¹è¯ï¼š
- æ¨¡å¼ 1ï¼šå…¨é‡é‡çˆ¬ï¼ˆqid/title/text/audio1/audio2/type/dateï¼‰
- æ¨¡å¼ 2ï¼šä»…è¡¥ type/dateï¼ˆåŸºäºå·²æœ‰ result.xlsxï¼‰
- ä¿ç•™ï¼šæ—¥å¿—è¾“å‡ºã€Ctrl-C å®¹é”™ã€SAVE_EVERY_PAGE å¢é‡è½ç›˜
"""

import os, re, time, sys, traceback, requests, pandas as pd
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from tqdm import tqdm

# ============ åŸºæœ¬é…ç½® ============ #
BASE_LIST_URL = "https://cclcat.com/dialogs?page={}"  # page=1..15
PAGES = range(1, 16)
COOKIE_STR = (
    "__Host-authjs.csrf-token=4aa46f7b20e87571898e527f534738ccc1516a5038c9d5922ef0d918ba93942c%7Cfd4a1367ffc6636624010509e8654b7e10d4681487bdb9c4238bee0f2917f336; __Secure-authjs.callback-url=https%3A%2F%2Fcclcat.com%2Fdialogs; __Secure-authjs.session-token=eyJhbGciOiJkaXIiLCJlbmMiOiJBMjU2Q0JDLUhTNTEyIiwia2lkIjoiVmp5T3gzY2Ryb0VBSkJkRkY5d3k0eDVtQXRBZzFkeUZMQ01MWWFvZzF0eGlldzNSUmNfLUJaLVBQb19DMHotTWN5bGFtWVNncThCS0h3bWowV0JrZmcifQ..p63woJDKyRA97cR8Yy54dw.u3IEjDwTM13SNe3E5xAwKUF8rkKZvubU5Fb-vOUai8Z_KFD_LfEEkwMAr1bdN03QJhDKAxa84FxP2aVdwHA-3uW6h7VPS_6JAagGXa7cc9KR5wDki-1ayPlplRaZZQcg3RJ6TNa3NN_DiKin3xoa8oDcR43oyUzHFCGYvRMfIYmWmqZeKVmz8jO3De-Sk2Ufqal9K9rxEc6rDg9It5GXbo1Auk86weHgBeGbPqYoR5bwmjeQpxzIG6N8dXd_l8PvazZ-Iq6WYEL6eHxvxsBGHYAFHn0GVFpCA1trf1DBqVrVDq4iNhiTVRq5uCYDYHzCdUdxOuz2daWR6KuyMiKlv71RovzwH9VVsUEn6A0VcSvw0-Iil-cgO8WW9-fs_vsF.e2TkR8ZDauLBS0mtZVXcqd6EvJJIm0tDRaUQ1d0bWvk"
)
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/126.0 Safari/537.36",
    "Cookie": COOKIE_STR,
}
TIMEOUT = 20
RETRY = 3
SAVE_EVERY_PAGE = False  # True â†’ æ¯çˆ¬å®Œä¸€é¡µç«‹å³å†™ Excel
# ---------- é€‰æ‹©å™¨ ---------- #
SEL_TEXT = ("span.ml-2.select-none.whitespace-pre-wrap.text-wrap."
            "text-left.text-sm.text-gray-800")
SEL_TITLE = "span.line-clamp-2.font-bold"
SEL_QID = "span.shrink-0.text-sm"
SEL_TYPE = "span.bg-lime-100.text-lime-600"  # ç±»åˆ«
SEL_DATE = "span.bg-amber-100.text-amber-600"  # æ—¥æœŸ
EXTRA_MENTION = "div.bg-yellow-200 p.text-sm" # é¢å¤–æç¤º

A_FILTER = ("peer", "absolute")  # åˆ—è¡¨é¡µ <a> å¿…é¡»å«çš„ç±»
# -------------------------------- #

DL_ROOT = "downloads"
EXCEL = "result.xlsx"
os.makedirs(DL_ROOT, exist_ok=True)


def slugify(txt, maxlen=40):
    return re.sub(r"[^\w\-]+", "_", txt).strip("_")[:maxlen] or "audio"


# ---------- åˆ—è¡¨é¡µ ---------- #
def list_dialog_urls(page_url: str) -> list[str]:
    html = requests.get(page_url, headers=HEADERS, timeout=TIMEOUT).text
    soup = BeautifulSoup(html, "html.parser")

    def cls_ok(c): return c and all(k in c.split() for k in A_FILTER)

    return [urljoin(page_url, a["href"]) for a in soup.find_all("a", class_=cls_ok) if a.get("href")]


# ---------- ä¸‹è½½å·¥å…· ---------- #
def download_file(url: str, path: str) -> bool:
    for attempt in range(1, RETRY + 1):
        try:
            with requests.get(url, headers=HEADERS, stream=True, timeout=TIMEOUT) as r:
                r.raise_for_status()
                total = int(r.headers.get("content-length", 0))
                with open(path, "wb") as f, tqdm(total=total, unit="B", unit_scale=True,
                                                 desc=os.path.basename(path), leave=False) as bar:
                    for chunk in r.iter_content(8192):
                        f.write(chunk);
                        bar.update(len(chunk))
            return True
        except Exception as e:
            print(f"  âš ï¸  ç¬¬{attempt}æ¬¡ä¸‹è½½å¤±è´¥ï¼š{e}")
            time.sleep(2)
    return False


# ---------- å¯¹è¯é¡µ ---------- #
def crawl_dialog(url: str, need_audio=True) -> list[dict]:
    """è¿”å›è¯¥é¡µæ‰€æœ‰è¡Œã€‚need_audio=False æ—¶åªæŠ“ type/dateï¼Œä¸ä¸‹è½½éŸ³é¢‘"""
    print(f"\nâ†» å¯¹è¯é¡µ {url}")
    html = requests.get(url, headers=HEADERS, timeout=TIMEOUT).text
    soup = BeautifulSoup(html, "html.parser")
    t_spans = soup.select(SEL_TEXT)

    # åŒä¸€é¡µ type/date ç›¸åŒï¼Œæå‰å–
    type_txt = soup.select_one(SEL_TYPE)
    date_txt = soup.select_one(SEL_DATE)
    extra_mention_txt = soup.select_one(EXTRA_MENTION)

    type_txt = type_txt.get_text(strip=True) if type_txt else ""
    date_txt = date_txt.get_text(strip=True) if date_txt else ""
    extra_mention_txt = extra_mention_txt.get_text(strip=True) if extra_mention_txt else ""

    rows = []
    for idx, t in enumerate(t_spans, 1):
        text = t.get_text(strip=True)
        title = t.find_previous("span", class_=lambda c: c and "line-clamp-2" in c.split())
        qid = t.find_previous("span", class_=lambda c: c and "shrink-0" in c.split())
        qid = (qid.get_text(strip=True) if qid else "").rstrip(".")

        aud1 = aud2 = ""
        if need_audio:
            # æ‰¾ä¸¤æ®µéŸ³é¢‘
            audios, node = [], t
            while len(audios) < 2 and node:
                node = node.find_next(["source", "span", "div"])
                if node and node.name == "source" and node.get("src"):
                    audios.append(urljoin(url, node["src"]))

            qdir = os.path.join(DL_ROOT, qid);
            os.makedirs(qdir, exist_ok=True)
            rels = []
            for i, src in enumerate(audios, 1):
                ext = os.path.splitext(urlparse(src).path)[-1] or ".m4a"
                fname = f"{idx:03d}_{i}_{slugify(text)}{ext}"
                fpath = os.path.join(qdir, fname)
                rels.append(f"/{qid}/{fname}" if download_file(src, fpath) else "")
            aud1, aud2 = (rels + ["", ""])[:2]

        print(f"[{idx:03d}] {qid} | {title.get_text(strip=True)[:20] if title else ''} | {text[:20]}")
        rows.append(dict(
            qid=qid, title=title.get_text(strip=True) if title else "",
            text=text, audio1=aud1, audio2=aud2,
            type=type_txt, date=date_txt,
            extraMention=extra_mention_txt,
        ))
    return rows


# ---------- Excel ---------- #
def save_rows(rows):
    df = pd.DataFrame(rows)
    cols = ["qid", "title", "text", "audio1", "audio2", "type", "date", "extraMention"]
    df = df.reindex(columns=cols)
    df.to_excel(EXCEL, index=False)
    print(f"ğŸ’¾ å·²ä¿å­˜ {len(df)} æ¡ â†’ {EXCEL}")


# ================= ä¸»æµç¨‹ ================= #
if __name__ == "__main__":
    mode = input("é€‰æ‹©æ¨¡å¼ (1=å…¨é‡é‡çˆ¬, 2=ä»…è¡¥ type/date): ").strip()
    if mode not in ("1", "2"):
        sys.exit("âŒ å¿…é¡»è¾“å…¥ 1 æˆ– 2")

    dialog_urls = []
    for p in PAGES:
        urls = list_dialog_urls(BASE_LIST_URL.format(p))
        print(f"ğŸ“„ page {p:02d} â†’ {len(urls)} æ¡")
        dialog_urls.extend(urls)

    all_rows = []

    try:
        if mode == "1":
            # â€”â€” å…¨é‡é‡çˆ¬ â€”â€” #
            for idx, url in enumerate(dialog_urls, 1):
                all_rows.extend(crawl_dialog(url, need_audio=True))
                if SAVE_EVERY_PAGE:
                    save_rows(all_rows)

        else:
            # â€”â€” ä»…è¡¥ type/date â€”â€” #
            if not os.path.exists(EXCEL):
                sys.exit("âŒ æœªæ‰¾åˆ° result.xlsxï¼Œæ— æ³•è¡¥å……")

            # 1) æŠŠ qid å¼ºåˆ¶è¯»æˆå­—ç¬¦ä¸²ï¼›é¡ºä¾¿æŠŠç©ºä¸²å¤„ç†æˆ NaN æ–¹ä¾¿åˆ¤æ–­
            # å¼ºåˆ¶å°†qidåˆ—è¯»å–ä¸ºstrï¼Œé¿å…æ•°æ®ç±»å‹é—®é¢˜
            df = pd.read_excel(EXCEL, dtype={"qid": str}).fillna("")

            # 2) åªæŒ‘å‡º type æˆ– date æˆ– extraMention ç¼ºå¤±çš„è¡Œ
            mask_need = (df["type"] == "") | (df["date"] == "") | (df["extraMention"] == "")

            q_need = set(df.loc[mask_need, "qid"])
            print(f"ğŸ‘‰ å…±æœ‰ {len(df)} æ¡è®°å½•ï¼Œå…¶ä¸­ {mask_need.sum()} æ¡ç¼ºå°‘ type/dateï¼Œå¼€å§‹è¡¥çˆ¬â€¦")

            fetched = {}  # qid â†’ (type, date)
            hit_total = 0

            for url in dialog_urls:
                page_hit = 0
                print(f"\nğŸŒ æ‰«æé¡µé¢ï¼š{url}")
                for row in crawl_dialog(url, need_audio=False):

                    qid = row["qid"]

                    print("aaaÃ aaaaaaaaaaaaaaaaaa")
                    print(qid)

                    if qid in q_need:
                        fetched[qid] = (row["type"], row["date"], row["extraMention"])
                        page_hit += 1
                        hit_total += 1
                        print(f"  âœ” å‘½ä¸­ qid={qid:<10} "
                              f"type='{row['type']}'  date='{row['date']}' extraMention='{row['extraMention']}'")
                print(f"  â€” é¡µé¢å‘½ä¸­ {page_hit} æ¡")

            print(f"\nâœ… æŠ“å–å®Œæ¯•ï¼Œç´¯è®¡è¡¥åˆ° {hit_total} æ¡ï¼ˆdict å¤§å°={len(fetched)}ï¼‰")


            # 3) æŠŠæŠ“åˆ°çš„å†…å®¹å†™å›â€”â€”åªè¦†ç›–åŸæœ¬ç¼ºå¤±çš„ä½ç½®
            def fill(col_idx):  # 0 -> type, 1 -> date
                return df.loc[mask_need, "qid"].map(lambda q: fetched.get(q, ("", "", ""))[col_idx])


            df.loc[mask_need, "type"] = fill(0)
            df.loc[mask_need, "date"] = fill(1)
            df.loc[mask_need, "extraMention"] = fill(2)

            # 4) ç»Ÿè®¡æ”¶å°¾
            left_type = df["type"].isna().sum()
            left_date = df["date"].isna().sum()
            left_extra = df["extraMention"].isna().sum()
            print(f"ğŸ” è¡¥å®Œåä»å‰© type ç¼ºå¤± {left_type} æ¡ã€date ç¼ºå¤± {left_date} æ¡, extraMention ç¼ºå¤± {left_extra}æ¡")

            # 5) ä¿å­˜ï¼ˆç”¨ä¸´æ—¶æ–‡ä»¶ä¿è¯å®‰å…¨ï¼‰
            tmp = "result_tmp.xlsx"
            df.to_excel(tmp, index=False)
            os.replace(tmp, EXCEL)
            print(f"ğŸ’¾ å·²ä¿å­˜åˆ° {EXCEL}")
            all_rows = df.to_dict("records")

    except KeyboardInterrupt:
        print("\nâ¹ï¸  æ‰‹åŠ¨ç»ˆæ­¢ï¼Œä¿å­˜å·²æŠ“å†…å®¹â€¦")
    except Exception as e:
        print("\nğŸ’¥  å¼‚å¸¸ï¼š", e)
        traceback.print_exc()
    finally:
        save_rows(all_rows)
        print("ğŸ ç»“æŸï¼ŒéŸ³é¢‘æ–‡ä»¶ä½äº downloads/{qid}/")
